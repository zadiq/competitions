{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ada861a85e9549dca27667692da408c5fdccbaa5"
   },
   "source": [
    "# About\n",
    "I learned a lot from Peter's kernal: [U-net, dropout, augmentation, stratification](https://www.kaggle.com/phoenigs/u-net-dropout-augmentation-stratification). \n",
    "\n",
    "I added more visualization to understand the analysis process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout, BatchNormalization,UpSampling2D,Concatenate\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "962c2c6775b5fcf605df8e7c59cbcabe6ba9ceaa"
   },
   "source": [
    "# Params and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e54e151245d665e42bb95d9cf2e1a33cb9440e48",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_size_ori = 101\n",
    "img_size_target = 128\n",
    "\n",
    "def upsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n",
    "    #res[:img_size_ori, :img_size_ori] = img\n",
    "    #return res\n",
    "    \n",
    "def downsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n",
    "    #return img[:img_size_ori, :img_size_ori]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "530c358f2868a444e8233936996463a66c2cc4f3"
   },
   "source": [
    "# Loading of training/testing ids and depths\n",
    "Reading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "24d7f3d982bfa582b222f012129acdda55282b6d"
   },
   "source": [
    "# Read images and masks\n",
    "Load the images and masks into the DataFrame and divide the pixel values by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b18c1f50cefd7504eae7e7b9605be3814c7cad6d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "86620c6a070571895f4f36ec050a25803915ed74",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1137f0a009f10b5f69e4dade5f689e744e9ce1d6"
   },
   "source": [
    "# Calculating the salt coverage and salt coverage classes\n",
    "Counting the number of salt pixels in the masks and dividing them by the image size. Also create 11 coverage classes, -0.1 having no salt at all to 1.0 being salt only.\n",
    "Plotting the distribution of coverages and coverage classes, and the class against the raw coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "18d2aa182a44c65a87c75f41047c653a79bc1c3f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b13d1ecc7004832e8e042d034922796263054b7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "        \n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5e66ff4809ea2f9a679b7ddbda5028dc324137a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "sns.distplot(train_df.coverage, kde=False, ax=axs[0])\n",
    "sns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\n",
    "plt.suptitle(\"Salt coverage\")\n",
    "axs[0].set_xlabel(\"Coverage\")\n",
    "axs[1].set_xlabel(\"Coverage class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2391c568019151b098a002937516bb77a506f403"
   },
   "source": [
    "# Plotting the depth distributions\n",
    "Separatelty plotting the depth distributions for the training and the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ae7b7011b7de3caed58f9ca3939df15ffa319ad",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(train_df.z, label=\"Train\")\n",
    "sns.distplot(test_df.z, label=\"Test\")\n",
    "plt.legend()\n",
    "plt.title(\"Depth distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14835b3e0eafd3a1c0e3a1f18a2e7979e75d3fa3"
   },
   "source": [
    "# Show some example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a6bc85ee458f72c0917edf77895d5abc5eaf3ee",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_images = 25\n",
    "grid_width = 5\n",
    "grid_height = int(max_images / grid_width)*2\n",
    "fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*4, grid_height*4))\n",
    "for i, idx in enumerate(train_df.index[:max_images]):\n",
    "    img = train_df.loc[idx].images\n",
    "    mask = train_df.loc[idx].masks\n",
    "    ax_image = axs[int(i / grid_width)*2, i % grid_width]\n",
    "    ax_image.imshow(img, cmap=\"Greys\")\n",
    "    ax_image.set_title(\"Image {0}\\nDepth: {1}\".format(i, train_df.loc[idx].z))\n",
    "    ax_image.set_yticklabels([])\n",
    "    ax_image.set_xticklabels([])\n",
    "    ax_mask = axs[int(i / grid_width)*2+1, i % grid_width]\n",
    "    ax_mask.imshow(img, cmap=\"Greys\")\n",
    "    ax_mask.imshow(mask, alpha=0.2, cmap=\"Greens\")\n",
    "    ax_mask.set_title(\"Mask {0}\\nCoverage: {1}\".format(i,  round(train_df.loc[idx].coverage, 2)))\n",
    "    ax_mask.set_yticklabels([])\n",
    "    ax_mask.set_xticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "00655e32f93f96ebd90dbe94e35ee052f52217cd"
   },
   "source": [
    "# Create train/validation split stratified by salt coverage\n",
    "Using the salt coverage as a stratification criterion. Also show an image to check for correct upsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2d3c3157512d11e71ac74ce51a937b85bedfe1d1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n",
    "    train_df.index.values,\n",
    "    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    train_df.coverage.values,\n",
    "    train_df.z.values,\n",
    "    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "63ac58ab47921b4e4f54102e2c8b85fa318225f1"
   },
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a517622135321d17e4aaad749def999205da358c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def build_model(input_layer, start_neurons):\n",
    "    # 128 -> 64\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    pool1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    # 64 -> 32\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    pool2 = Dropout(0.5)(pool2)\n",
    "\n",
    "    # 32 -> 16\n",
    "    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    pool3 = Dropout(0.5)(pool3)\n",
    "\n",
    "    # 16 -> 8\n",
    "    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n",
    "    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(0.5)(pool4)\n",
    "\n",
    "    # Middle\n",
    "    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n",
    "    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(convm)\n",
    "\n",
    "    # 8 -> 16\n",
    "    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv4 = concatenate([deconv4, conv4])\n",
    "    uconv4 = Dropout(0.5)(uconv4)\n",
    "    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n",
    "    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n",
    "\n",
    "    # 16 -> 32\n",
    "    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n",
    "    uconv3 = concatenate([deconv3, conv3])\n",
    "    uconv3 = Dropout(0.5)(uconv3)\n",
    "    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n",
    "    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n",
    "\n",
    "    # 32 -> 64\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    uconv2 = Dropout(0.5)(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "\n",
    "    # 64 -> 128\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    uconv1 = Dropout(0.5)(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "\n",
    "    ucov1 = Dropout(0.5)(uconv1)\n",
    "    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n",
    "    \n",
    "    return output_layer\n",
    "\n",
    "input_layer = Input((img_size_target, img_size_target, 1))\n",
    "output_layer = build_model(input_layer, 16)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "844cf209ff2d6742583b86f5f4770efa90bb5d70",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block(m, dim, acti, bn, res, do=0):\n",
    "    n = Conv2D(dim, 3, activation=acti, padding='same')(m)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    n = Dropout(do)(n) if do else n\n",
    "    n = Conv2D(dim, 3, activation=acti, padding='same')(n)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    return Concatenate()([m, n]) if res else n\n",
    "\n",
    "def level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n",
    "    if depth > 0:\n",
    "        n = conv_block(m, dim, acti, bn, res)\n",
    "        m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
    "        m = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n",
    "        if up:\n",
    "            m = UpSampling2D()(m)\n",
    "            m = Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
    "        else:\n",
    "            m = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n",
    "        n = Concatenate()([n, m])\n",
    "        m = conv_block(n, dim, acti, bn, res)\n",
    "    else:\n",
    "        m = conv_block(m, dim, acti, bn, res, do)\n",
    "    return m\n",
    "\n",
    "def UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n",
    "         dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n",
    "    i = Input(shape=img_shape)\n",
    "    o = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
    "    o = Conv2D(out_ch, 1, activation='sigmoid')(o)\n",
    "    return Model(inputs=i, outputs=o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1aa78bd7c607e1f0e0235e4b2f82056c0361dac5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = Model(input_layer, output_layer)\n",
    "model = UNet((img_size_target,img_size_target,1),start_ch=16,depth=5,batchnorm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3399029adb039b049e3d6ca01fef30ed8653482b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c7ded4adc1757c88a1bea59ea36b1a9f7941bd28",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c007157c2fd3d7dadcaeee2a6376351852d1e565"
   },
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88b3f57eac3ec3719b401730dc6d8d2d89d09ccc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n",
    "y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7040f72549212dd4f71c13dfbd8bf013481ea369",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 10, figsize=(15,3))\n",
    "for i in range(10):\n",
    "    axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n",
    "    axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n",
    "    axs[1][i].imshow(x_train[int(len(x_train)/2 + i)].squeeze(), cmap=\"Greys\")\n",
    "    axs[1][i].imshow(y_train[int(len(y_train)/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\n",
    "fig.suptitle(\"Top row: original images, bottom row: augmented images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f5a6b1abaa4681cba3b608bc5f33cf260370d82a"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f1773642758da7b4480e0e48c045bd01ea3684ae",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=10, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(\"./keras.model\", save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.000005, verbose=1)\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=[x_valid, y_valid], \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42e9ef3c4e0a2bb2539e5e51740ba6bfc092d37c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "ax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax_loss.legend()\n",
    "ax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n",
    "ax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")\n",
    "ax_acc.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c824f6bca47f051500966c433ce7fb5a9528f6d7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = load_model(\"./keras.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0f168318eadb324daa8c020f0e3e0a24d82a464f"
   },
   "source": [
    "# Predict the validation set to do a sanity check\n",
    "Again plot some sample images including the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "16cbfe2fee11a8b13b96ce78161ce19b5e5a0c46",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\n",
    "preds_valid = np.array([downsample(x) for x in preds_valid])\n",
    "mask_valid = np.array([downsample(x) for x in y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fd973023204ebf921fe1f23748856e6a6f692aa4",
    "collapsed": true
   },
   "source": [
    "# Scoring\n",
    "Score the model and do a threshold optimization by the best IoU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d261beec66b6867ac0d5c94684f12aa08b70d638",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# src: https://www.kaggle.com/aglotero/another-iou-metric\n",
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = y_true_in\n",
    "    y_pred = y_pred_in\n",
    "    \n",
    "    true_objects = 2\n",
    "    pred_objects = 2\n",
    "\n",
    "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins = true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    return np.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "85f6d9567cec0ef8976730a6834b6569b6e108a0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 50)\n",
    "ious = np.array([iou_metric_batch(mask_valid, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "183d37ad32bc2f1f0d17a9538702c45a826ccefc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold_best_index = np.argmax(ious[9:-10]) + 9\n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ced29761f2d1760245112a30a7abd4783b373dd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(thresholds, ious)\n",
    "plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "423b3268c580dc1eae84f54deeeb0f691eff6028"
   },
   "source": [
    "# Sanity check with adjusted threshold\n",
    "Again some sample images with the adjusted threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "40c263765ac6d53a8c0c1361ff1e6f061eecf825",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_images = 30\n",
    "grid_width = 5\n",
    "grid_height = int(max_images / grid_width)*3\n",
    "fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*4, grid_height*4))\n",
    "for i, idx in enumerate(ids_valid[:max_images]):\n",
    "    #print(idx)\n",
    "    img = downsample(np.squeeze(x_valid[i]))\n",
    "    mask = np.squeeze(mask_valid[i])\n",
    "    pred = np.squeeze(preds_valid[i]>threshold_best)\n",
    "    ax_image = axs[int(i / grid_width)*3, i % grid_width]\n",
    "    ax_image.imshow(img, cmap=\"Greys\")\n",
    "    ax_image.set_title(\"Image {0}\\nDepth: {1}\".format(idx, train_df.loc[idx].z))\n",
    "    ax_image.set_yticklabels([])\n",
    "    ax_image.set_xticklabels([])\n",
    "    ax_mask = axs[int(i / grid_width)*3+1, i % grid_width]\n",
    "    ax_mask.imshow(img, cmap=\"Greys\")\n",
    "    ax_mask.imshow(mask, alpha=0.9, cmap=\"Greens\")\n",
    "    ax_mask.set_title(\"Mask {0}\\nCoverage: {1}\".format(idx,  round(train_df.loc[idx].coverage, 2)))\n",
    "    ax_mask.set_yticklabels([])\n",
    "    ax_mask.set_xticklabels([])\n",
    "    ax_pred = axs[int(i / grid_width)*3+2, i % grid_width]\n",
    "    ax_pred.imshow(img, cmap=\"Greys\")\n",
    "    ax_pred.imshow(pred, alpha=0.9, cmap=\"Blues\")\n",
    "    coverage_pred = np.sum(pred) / pow(img_size_ori, 2)\n",
    "    ax_pred.set_title(\"Predict {0}\\nCoverage: {1}\".format(idx,  round(coverage_pred, 2)))\n",
    "    ax_pred.set_yticklabels([])\n",
    "    ax_pred.set_xticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9171f2e7255a8a6deb07b0ceb131ca4fa9007e57",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot small charts\n",
    "max_images = 24\n",
    "grid_width = 12\n",
    "grid_height = int(max_images / grid_width)*3\n",
    "fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*2, grid_height*2))\n",
    "for i, idx in enumerate(ids_valid[:max_images]):\n",
    "    #print(idx)\n",
    "    img = downsample(np.squeeze(x_valid[i]))\n",
    "    mask = np.squeeze(mask_valid[i])\n",
    "    pred = np.squeeze(preds_valid[i]>threshold_best)\n",
    "    ax_image = axs[int(i / grid_width)*3, i % grid_width]\n",
    "    ax_image.imshow(img, cmap=\"Greys\")\n",
    "    ax_image.set_title(\"Image\")\n",
    "    ax_image.set_yticklabels([])\n",
    "    ax_image.set_xticklabels([])\n",
    "    ax_mask = axs[int(i / grid_width)*3+1, i % grid_width]\n",
    "    ax_mask.imshow(img, cmap=\"Greys\")\n",
    "    ax_mask.imshow(mask, alpha=0.9, cmap=\"Greens\")\n",
    "    ax_mask.set_title(\"Mask\")\n",
    "    ax_mask.set_yticklabels([])\n",
    "    ax_mask.set_xticklabels([])\n",
    "    ax_pred = axs[int(i / grid_width)*3+2, i % grid_width]\n",
    "    ax_pred.imshow(img, cmap=\"Greys\")\n",
    "    ax_pred.imshow(pred, alpha=0.9, cmap=\"Blues\")\n",
    "    coverage_pred = np.sum(pred) / pow(img_size_ori, 2)\n",
    "    ax_pred.set_title(\"Predict\")\n",
    "    ax_pred.set_yticklabels([])\n",
    "    ax_pred.set_xticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "332a614c0ae837c115ec6563f355753ffbb8cd83"
   },
   "source": [
    "# Submission\n",
    "Load, predict and submit the test image predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "72128add82c6853441671fde67e7e66601a01787",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source https://www.kaggle.com/bguberfain/unet-with-depth\n",
    "def RLenc(img, order='F', format=True):\n",
    "    \"\"\"\n",
    "    img is binary mask image, shape (r,c)\n",
    "    order is down-then-right, i.e. Fortran\n",
    "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
    "\n",
    "    returns run length as an array or string (if format is True)\n",
    "    \"\"\"\n",
    "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
    "    runs = []  ## list of run lengths\n",
    "    r = 0  ## the current run length\n",
    "    pos = 1  ## count starts from 1 per WK\n",
    "    for c in bytes:\n",
    "        if (c == 0):\n",
    "            if r != 0:\n",
    "                runs.append((pos, r))\n",
    "                pos += r\n",
    "                r = 0\n",
    "            pos += 1\n",
    "        else:\n",
    "            r += 1\n",
    "\n",
    "    # if last run is unsaved (i.e. data ends with 1)\n",
    "    if r != 0:\n",
    "        runs.append((pos, r))\n",
    "        pos += r\n",
    "        r = 0\n",
    "\n",
    "    if format:\n",
    "        z = ''\n",
    "\n",
    "        for rr in runs:\n",
    "            z += '{} {} '.format(rr[0], rr[1])\n",
    "        return z[:-1]\n",
    "    else:\n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ecb152b492c7126d12c5ef2c701eec8ea3d86f1",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_test = np.array([upsample(np.array(load_img(\"../input/test/images/{}.png\".format(idx), grayscale=True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f278d0b87320c117b4ed7c116a991782b82ba5a7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_test = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "113f816f9db8b87ca7f6845fe6e61328ab606f41",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4243166f91c4bcb4da00208f4f53dd912dbb429f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub.to_csv('submission.csv')\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "print('Submission output to: sub-{}.csv'.format(timestamp))\n",
    "sub.to_csv(\"sub-{}.csv\".format(timestamp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd6ce9b4d5fc80a2502a43e80299d628fb5ffc42",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
